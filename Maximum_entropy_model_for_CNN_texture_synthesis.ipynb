{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Maximum_entropy_model_for_CNN_texture_synthesis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgalerne/mva_generative_models_for_images/blob/main/Maximum_entropy_model_for_CNN_texture_synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Maximum Entropy Model for CNN Texture Synthesis"
      ],
      "metadata": {
        "id": "8Uipncc7H96g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This practical session implements the texture synthesis algorithm developed in the paper **Maximum entropy methods for texture synthesis: theory and practice** by *V. De Bortoli, A. Desolneux, A. Durmus, B. Galerne, A. Leclaire*.\n",
        "\n",
        "**References:**\n",
        "\n",
        "* Paper: Maximum entropy methods for texture synthesis: theor y and practice,  V. De Bor toli, A. Desolneux, A. Dur mus, B. Galerne, A. Leclaire, SIAM Jour nal on Mathematics of Data Science (SIMODS), 2021\n",
        "\n",
        "* Public repository: https://gitlab.com/vdeborto/macrocanonical-synthesis/-/tree/master/\n",
        "\n",
        "**Authors:**\n",
        "\n",
        "* Bruno Galerne: www.idpoisson.fr/galerne / https://github.com/bgalerne\n",
        "* Luc√≠a Bouza\n",
        "\n",
        "\n",
        "**Texture Synthesis:** Given an input texture image, produce an output texture image being both visually similar to and pixel-wise different from the input texture. The output image should ideally be perceived as another part of the same large piece of homogeneous material the input texture is taken from.\n",
        "\n"
      ],
      "metadata": {
        "id": "2apo4YEvaI4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Underlying Principle"
      ],
      "metadata": {
        "id": "-vE81vJEijcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Framework: Macrocanonical model**. \n",
        "\n",
        "One looks for the parameters $\\theta\\in\\mathbb{R}^p$ \n",
        "such that the exponential model\n",
        "$$\n",
        "\\pi_{\\theta}(x) \\propto e^{-V(x,\\theta)} dx\n",
        "$$\n",
        "where\n",
        "$$ \n",
        "V(x,\\theta) = \\theta \\cdot (f(x)-f(x_0)) + J(x)\n",
        "$$\n",
        "with\n",
        "\n",
        "* $x_0$ is the target texture, \n",
        "\n",
        "* $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ is the spatial average of the feature responses of each selected layers (multiplied by $\\beta = 128$).\n",
        "\n",
        "* $J(x) = \\frac{\\epsilon}{2}  \\left\\| x \\right\\|^2$\n",
        "\n",
        "such that $\\theta$ is a solution of the macrocanonical problem, that is, \n",
        "$$\n",
        "\\mathbb{E}_{\\pi_{\\theta}}[f(X)] = f(x_0)\n",
        "$$\n",
        "and $\\pi_{\\theta}$ has maximal entropy.\n",
        "\n",
        "Let us recall that the pseudo-code of the algorithm.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DxdZIVewbLvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SOUL algorithm**\n",
        "\n",
        "* Initialization: $\\theta \\leftarrow 0$; $X_0^0 \\in \\mathbb{R}^d$\n",
        "* For $n = 1, \\ldots, N$,\n",
        "\n",
        "  * $m_n$ steps of Langevin diffusion: for $k=0,\\ldots,m_n-1$, \n",
        "         \n",
        "  $$\n",
        "  X_{k+1}^n = X_k^n - \\gamma_{n+1} \\nabla_x V(X_k^n,\\theta_n) + \\sqrt{2\\gamma_{n+1}} Z_{k+1}^n\n",
        "  $$\n",
        "  with $Z_{k+1}^n \\sim \\mathcal{N}(0,I)$\n",
        "\n",
        " * Update $\\theta$ with Langevin intermediary states:\n",
        "\n",
        "  $$\n",
        "  \\theta_{n+1} = \\mathsf{Proj}_{\\Theta}\\left( \\theta_n + \\frac{\\delta_{n+1}}{m_n} \\sum_{k=1}^{m_n} f(X_k^n) - f(x_0)\\right)\n",
        "  $$\n",
        "\n",
        " * Set warm start for next step: $X_0^{n+1} = X_{m_n}^n$\n",
        "\n",
        "\n",
        "**In Practice:**\n",
        "* The initialization $X_0^0$ is an ADSN realization (see below).\n",
        "* We do not use projections.\n",
        "* $\\epsilon = 0.1$\n",
        "* $\\gamma$ and $\\delta$ are fixed.\n",
        "* $m = 1$, so we do one update of $x$, one update of $\\theta$, and so on. \n",
        "* Use the layers [1, 3, 6, 8, 11, 13, 15, 24, 26, 31] for optimization."
      ],
      "metadata": {
        "id": "s6iZrtsSKgiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages ##\n",
        "\n",
        "Below is a list of the packages needed to implement the texture synthesis.\n",
        "\n",
        "\n",
        "\n",
        "* `torch` (indispensables packages for neural networks with PyTorch)\n",
        "* `torchvision.transforms.functional` (necessary to transform images into tensors)\n",
        "* `torchvision.models` (for get vgg network)\n",
        "* `PIL.Image, matplotlib.pyplot, os, display` (load and display images)"
      ],
      "metadata": {
        "id": "duLUeb29WLj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import mse_loss\n",
        "import torchvision.models as models\n",
        "from torchvision.transforms.functional import resize, to_tensor, normalize, to_pil_image\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NUFaAOU0cGzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Images\n",
        "\n",
        "On next section we will load images. Here we will just get and display the images, without doing any changes to it. "
      ],
      "metadata": {
        "id": "O1BBDx-OWPKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texture_imgnames = [\"bois.png\", \"briques.png\", \"mur.png\", \"tissu.png\", \n",
        "                    \"nuages.png\",\"pebbles.jpg\",\"wall1003.png\", \"osier12.png\",\n",
        "                    \"paille17c2.png\",\"bark.png\",\"coffee.png\",\"flower.png\",\n",
        "                    \"rock.png\",\"sweet.png\"]\n",
        "\n",
        "for fname in texture_imgnames:\n",
        "    os.system(\"wget -c https://www.idpoisson.fr/galerne/mva/\"+fname)\n",
        "    img = Image.open(fname)\n",
        "    print(fname)\n",
        "    display(img)"
      ],
      "metadata": {
        "id": "UEBuIZIBWTpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set a device\n",
        "\n",
        "Next, we need to choose which device to run the network. Running the algorithm on large images takes longer and will go much faster when running on a GPU. We can use `torch.cuda.is_available()` to detect if there is a GPU available. Next, we set the `torch.device`. Also the `.to(device)` method is used to move tensors or modules to a desired device."
      ],
      "metadata": {
        "id": "eCd7bbwyWYs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device is\", device)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "RYmgf5zcWfIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data ##\n",
        "\n",
        "The original PIL images have values between 0 and 255, but when transformed into torch tensors, their values are converted to be between 0 and 1. \n",
        "\n",
        "An important detail to note is that neural networks from the torch library are trained with tensor values ranging from 0 to 1. Additionally, VGG networks are trained on images with each channel normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] (mean and standard deviation of Imagenet). We will have to normalize the image tensor before sending it into the network.\n",
        "\n",
        "Here some auxiliary functions to load, display and transform to tensors. "
      ],
      "metadata": {
        "id": "-oKUGrkFWjd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities\n",
        "# Functions to manage images\n",
        "\n",
        "MEAN = (0.485, 0.456, 0.406)\n",
        "STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "def prep_img(image: str, size=None, mean=MEAN, std=STD):\n",
        "    \"\"\"Preprocess image.\n",
        "    1) load as PIl\n",
        "    2) resize\n",
        "    3) convert to tensor\n",
        "    5) remove alpha channel if any\n",
        "    4) normalize\n",
        "    \"\"\"\n",
        "    im = Image.open(image)\n",
        "    texture = resize(im, size)\n",
        "    texture_tensor = to_tensor(texture).unsqueeze(0)\n",
        "    if texture_tensor.shape[1]==4:\n",
        "        print('removing alpha chanel')\n",
        "        texture_tensor = texture_tensor[:,:3,:,:]\n",
        "    texture_tensor = normalize(texture_tensor, mean=mean, std=std)\n",
        "    return texture_tensor\n",
        "\n",
        "\n",
        "def denormalize(tensor: torch.Tensor, mean=MEAN, std=STD, inplace: bool = False):\n",
        "    \"\"\"Based on torchvision.transforms.functional.normalize.\n",
        "    \"\"\"\n",
        "    tensor = tensor.clone().squeeze() \n",
        "    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
        "    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
        "    tensor.mul_(std).add_(mean)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def to_pil(tensor: torch.Tensor):\n",
        "    \"\"\"Converts tensor to PIL Image.\n",
        "    Args: tensor (torch.Temsor): input tensor to be converted to PIL Image of torch.Size([C, H, W]).\n",
        "    Returns: PIL Image: converted img.\n",
        "    \"\"\"\n",
        "    img = tensor.clone().detach().cpu()\n",
        "    img = denormalize(img).clip(0, 1)\n",
        "    img = to_pil_image(img)\n",
        "    return img"
      ],
      "metadata": {
        "id": "xGazoe_hWn2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "Now we need to import a pretrained neural network. We will use as base the 19 layer VGG network of PyTorch.\n",
        "\n",
        "PyTorch implementation of VGG is a module divided into two child Sequential modules: features (containing convolution and pooling layers), and classifier (containing fully connected layers). For the texture synthesis task we only need the layers of the features module.\n"
      ],
      "metadata": {
        "id": "0N6iFqX0Wzzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "\n",
        "# Don't let parameters to change\n",
        "cnn.requires_grad_(False)"
      ],
      "metadata": {
        "id": "QJ2iflvTW3SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will slithgly change the network so that $f$ is differentiable. We will change Relu for CeLu and Max Pooling for Average Pooling.\n",
        "\n",
        "On the output of next commands you can see the structure of `features` module. Indexes will help to select the needed layers for the algorithm. "
      ],
      "metadata": {
        "id": "zaUxmPzlXO9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def differentiable(cnn):\n",
        "    \"\"\"\n",
        "    This function replaces non differentiable non-linear functions\n",
        "    in the network by differentiable ones.\n",
        "    \"\"\"\n",
        "\n",
        "    for i, layer in cnn.named_modules():\n",
        "        if isinstance(layer, nn.ReLU):\n",
        "            cnn[int(i)] = nn.CELU(inplace=True)\n",
        "        if isinstance(layer, nn.MaxPool2d):\n",
        "            cnn[int(i)] = nn.AvgPool2d(2, stride=2, padding=0, ceil_mode=False)\n",
        "\n",
        "# Replace no differentiable functions\n",
        "differentiable(cnn)"
      ],
      "metadata": {
        "id": "apYBaY-nWFLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the algorithm explained at the beginning of this notebook, we need to access the outputs of some selected intermediate layers.\n",
        "\n",
        "In order to access the outputs of the layers on PyTorch VGG19 network, we need to register a hook on each layer we need. Hooks are functions, able to be attached to every layer and called each time the layer is used. You can register a hook before or after the forward pass, or after the backward pass. We will define a function `save_output` that will be triggered after the forward pass, for each layer of `features` module. \n",
        "\n",
        "The outputs of the layers will be store on a dictionary where the key is the index of the layer and the value is the output tensor of the layer.\n",
        "\n",
        "So, we must define which layers will be part of the optimization. Using the indexes of layers, we select the layers to use in the algorithm. We will choose indexes [1, 3, 6, 8, 11, 13, 15, 24, 26, 31]. "
      ],
      "metadata": {
        "id": "kMhx1j-wW_X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize outputs dic\n",
        "outputs = {}\n",
        "\n",
        "# Hook definition\n",
        "def save_output(name):\n",
        "    \n",
        "    # The hook signature\n",
        "    def hook(module, module_in, module_out):\n",
        "        outputs[name] = module_out\n",
        "    return hook\n",
        "\n",
        "# Define layers\n",
        "layers = [1,3, 6, 8, 11, 13, 15, 24, 26, 31]\n",
        "\n",
        "# Register hook on each layer with index on array \"layers\"\n",
        "for layer in layers:\n",
        "    handle = cnn[layer].register_forward_hook(save_output(layer))"
      ],
      "metadata": {
        "id": "NOwbHXgAXDW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function $f$ corresponds to the spatial mean of the VGG-19 layers computed by the function ```mean_Spatial``` below."
      ],
      "metadata": {
        "id": "OFmqHQirE1KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for computing the spatial average of the feature responses of each selected layer\n",
        "def mean_Spatial (input: torch.Tensor):  \n",
        "    mean_input = torch.mean(input.squeeze(), axis=(1,2))  \n",
        "    return mean_input"
      ],
      "metadata": {
        "id": "dWiTFMV4fY9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization\n",
        "\n"
      ],
      "metadata": {
        "id": "po_c4hWefS6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the ADSN model to initialize the synthesis.\n",
        "\n",
        "**ADSN initialization:** \n",
        "\n",
        "Let $h\\in\\mathbb{R}^{M\\times N\\times 3}$ be a an image, $m = (m_r, m_g, m_b)$ be the mean color of $h$ and $X$ be a Gaussian white noise image.\n",
        "The random image\n",
        "$$\n",
        "Y = m + \\frac{1}{\\sqrt{MN}}\n",
        "\\begin{pmatrix}\n",
        "\\left( h_r - m_r \\right) \\ast X\\\\\n",
        "\\left( h_g - m_g \\right) \\ast X\\\\\n",
        "\\left( h_b - m_b \\right) \\ast X\n",
        "\\end{pmatrix},~~~X\\in\\mathbb{R}^{M\\times N}~\\text{a Gaussian white noise},\n",
        "$$\n",
        "is the ADSN associated with $h$."
      ],
      "metadata": {
        "id": "Od2KGrv72Qvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ADSN initialization. \n",
        "def adsn(input):\n",
        "    # input is supposed to be a tensor of size 1 x c x h x w\n",
        "    tnsr = input.squeeze(0)\n",
        "    c, h, w = tnsr.size() \n",
        "    m = torch.mean(tnsr, axis=(1,2))\n",
        "    X = torch.randn(h, w).to(device)\n",
        "    Y = torch.empty_like(tnsr)\n",
        "    sqrtHW = np.sqrt(h*w)\n",
        "\n",
        "    for i in range(c):\n",
        "        tnsrnorm = (tnsr[i,:,:]-m[i])/sqrtHW\n",
        "        Y[i,:,:] = torch.real(torch.fft.ifft2(torch.fft.fft2(tnsrnorm) * torch.fft.fft2(X))) + m[i]\n",
        "\n",
        "    return Y.unsqueeze(0)"
      ],
      "metadata": {
        "id": "0PczHND9vtPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "### This section allows you to change target image ###\n",
        "######################################################\n",
        "# Select input image: \n",
        "#  [\"bois.png\", \"briques.png\", \"mur.png\", \"tissu.png\", \n",
        "#   \"nuages.png\",\"pebbles.jpg\",\"wall1003.png\", \"osier12.png\",\n",
        "#   \"paille17c2.png\",\"bark.png\",\"coffee.png\",\"flower.png\",\n",
        "#   \"rock.png\",\"sweet.png\"]\n",
        "input_image_name = \"coffee.png\"#\"sweet.png\"#\"bark.png\"##\"rock.png\"#\"paille17c2.png\"\n",
        "img_size = 256\n",
        "\n",
        "# Prepare texture data\n",
        "target = prep_img(input_image_name, img_size).to(device)\n",
        "######################################################\n",
        "\n",
        "# set seed to reproduce examples\n",
        "torch.manual_seed(123)\n",
        "\n",
        "#init image with adsn from target image (normalized)\n",
        "x = adsn(target)\n",
        "\n",
        "# print images\n",
        "display(to_pil(torch.cat((target, x), axis=3)))\n",
        "plt.pause(0.05)"
      ],
      "metadata": {
        "id": "c5723ssWfYLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Texture Synthesis\n",
        "\n",
        "Finally, we must run code that performs the texture synthesis. For each iteration, we update the image $x$ and then update the weights $\\theta$.\n",
        "\n",
        "**Exercise:** \n",
        "1. Fill in the ```#TODO``` segments of the code to perform texture synthesis using the SOUL algorithm. The formula are rewritten below.\n",
        "2. What is the dimension $p$ here? \n",
        "2. What hapens when $\\gamma$ is inscreased?\n",
        "3. Is the ADSN initialization important?\n"
      ],
      "metadata": {
        "id": "nN-rZ4WHrh8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SOUL algorithm**\n",
        "\n",
        "* Initialization: $\\theta \\leftarrow 0$; $X_0^0 \\in \\mathbb{R}^d$ an ADSN realization\n",
        "* For $n = 1, \\ldots, N$,\n",
        "\n",
        "  * $m_n = 1$ step of Langevin diffusion: for $k=0,\\ldots,m_n-1$, \n",
        "         \n",
        "  $$\n",
        "  X_{k+1}^n = X_k^n - \\gamma_{n+1} \\nabla_x V(X_k^n,\\theta_n) + \\sqrt{2\\gamma_{n+1}} Z_{k+1}^n\n",
        "  $$\n",
        "  with $Z_{k+1}^n \\sim \\mathcal{N}(0,I)$\n",
        "\n",
        " * Update $\\theta$ with Langevin intermediary states:\n",
        "\n",
        "  $$\n",
        "  \\theta_{n+1} = \\mathsf{Proj}_{\\Theta}\\left( \\theta_n + \\frac{\\delta_{n+1}}{m_n} \\sum_{k=1}^{m_n} f(X_k^n) - f(x_0)\\right)\n",
        "  $$\n",
        "\n",
        " * Set warm start for next step: $X_0^{n+1} = X_{m_n}^n$\n",
        "\n",
        "\n",
        "**In Practice:**\n",
        "* The initialization $X_0^0$ is an ADSN realization (see below).\n",
        "* $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ is the spatial average of the feature responses of each selected layers **multiplied by $\\beta = 128$**.\n",
        "* We do not use projections.\n",
        "* $\\epsilon = 0.1$\n",
        "* $\\gamma$ and $\\delta$ are fixed (see value in code below).\n",
        "* $m = 1$, so we do one update of $x$, one update of $\\theta$, and so on. \n",
        "* Use the layers [1, 3, 6, 8, 11, 13, 15, 24, 26, 31] for optimization."
      ],
      "metadata": {
        "id": "PijtjhIaCDnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 3000\n",
        "log_every = n_iters//10\n",
        "\n",
        "# steps and epsilon\n",
        "delta = 10e-1\n",
        "gamma = 2*10e-6\n",
        "epsilon = 0.1\n",
        "\n",
        "# Compute just once means spatial of activations of the target image. \n",
        "cnn(target)\n",
        "meansTargetOutputs = [mean_Spatial(outputs[key]) for key in layers] \n",
        "\n",
        "# initialize weights (Theta)\n",
        "theta = [torch.zeros_like(meansTargetOutputs[i]) for i in range(len(meansTargetOutputs))]\n",
        "\n",
        "# Initialize list of intermediary images\n",
        "xpil_list = []\n",
        "\n",
        "# Forward pass using x. Get activations of selected layers for image x (outputs).\n",
        "x.requires_grad=True\n",
        "cnn(x)\n",
        "x_outputs = [outputs[key] for key in layers]\n",
        "\n",
        "for iter in range(n_iters):\n",
        "    \n",
        "    if x.grad is not None:\n",
        "        x.grad.zeros_()\n",
        "  \n",
        "    # Compute V and its gradient with respect to x:\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    # update image\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # TODO\n",
        "\n",
        "    # Forward pass using x. Get activations of selected layers for image x (outputs).\n",
        "    cnn(x)\n",
        "    x_outputs = [outputs[key] for key in layers] \n",
        "\n",
        "    # update weights thetas:\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(layers)):\n",
        "            \n",
        "            # TODO\n",
        "\n",
        "    # Display results: print Loss value and show image\n",
        "    if (iter==0 or iter % log_every == log_every-1):\n",
        "        print('Iteration: ', iter)\n",
        "        display(to_pil(torch.cat((target, x), axis=3)))\n",
        "        # Store for comparison:\n",
        "        xpil_list.append(to_pil(x.clone().detach()))"
      ],
      "metadata": {
        "id": "7RM-SHGZkROX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import widgets\n",
        "\n",
        "def compare_images(imgs):\n",
        "    labels = ['image ' + str(i) for i in range(len(imgs))]\n",
        "    tb = widgets.TabBar(labels, location='top')\n",
        "    for i, img in enumerate(imgs):\n",
        "        with tb.output_to(i, select=(i == 0)):\n",
        "            display(img)\n",
        "\n",
        "\n",
        "compare_images(xpil_list)\n"
      ],
      "metadata": {
        "id": "KKCqBlF2XBox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}