{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgalerne/mva_generative_models_for_images/blob/main/2_mvagm_CNN_texture_synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3aqjgrs4f6U"
      },
      "source": [
        "# Texture Synthesis with CNNs in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsZ7qPBk4tO0"
      },
      "source": [
        "## Introduction ##\n",
        "\n",
        "This practical session explains how to implement the Texture Synthesis based on the algorithm developed by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge on the Paper **[Texture Synthesis Using Convolutional Neural Networks](https://papers.nips.cc/paper/5633-texture-synthesis-using-convolutional-neural-networks)**. \n",
        "\n",
        "\n",
        "**Sources:**\n",
        "This practical session is based on several resources:\n",
        "\n",
        "*   Original code: https://github.com/leongatys/DeepTextures\n",
        "*   Reimplementation: https://github.com/trsvchn/deep-textures\n",
        "*   Tutorial used for some explanations: https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
        "\n",
        "**Authors:**\n",
        "* Bruno Galerne: www.idpoisson.fr/galerne / https://github.com/bgalerne\n",
        "* Lucía Bouza\n",
        "\n",
        "\n",
        "**Texture Synthesis**: Given an input texture image, produce an output texture image being both visually similar to and pixel-wise different from the input texture. The output image should ideally be perceived as another part of the same large piece of homogeneous material the input texture is taken from.\n",
        "\n",
        "\n",
        "\n",
        "##Underlying Principle##\n",
        "\n",
        "Let us recall the algorithm proposed by Gatys et al.\n",
        "Given an example image $u$ and a random initialization $x=x_0$, \n",
        "one optimizes the loss function \n",
        "$$\n",
        "E(x) = \\sum_{\\text{for selected layers } L} w_L\\left\\| G^L(x) - G^L(u) \\right\\|^2_F\n",
        "$$\n",
        "where $\\|\\cdot\\|_F$ is the Frobenius norm and for an image $y$ and a layer index $L$ $G^L(y)$ denotes the Gram matrix of the VGG-19 features at layer $L$:\n",
        "if $V^L(y)$ is the feature response of $y$ at layer $L$ that has spatial size $w\\times h$ and $n$ channels, \n",
        "$$\n",
        "G^L(y) = \\frac{1}{w h}\\sum_{k\\in \\{0,\\dots,w-1\\}\\times\\{0,\\dots,h-1\\}} V^L(y)_k V^L(y)_k^T \\in \\mathbb{R}^{n\\times n}.\n",
        "$$\n",
        "The optimization is done using the L-BFGS algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1:\n",
        "\n",
        "1. Go through the notebook and execute each cell.\n",
        "\n",
        "2. We are using the outputs of 5 VGG-19 layers to define $E$. Verify that the quality of the output texture decreases if one uses less layers (e.g. only the first layer or the three first layers).\n"
      ],
      "metadata": {
        "id": "v_rMWIwdsIfn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h81V-9U4EwN0"
      },
      "source": [
        "## Importing Packages ##\n",
        "\n",
        "Below is a list of the packages needed to implement the texture synthesis.\n",
        "\n",
        "\n",
        "\n",
        "* `torch` (indispensables packages for neural networks with PyTorch)\n",
        "* `torchvision.transforms.functional` (necessary to transform images into tensors)\n",
        "* `torchvision.models` (to get vgg network)\n",
        "* `mse_loss` (to compute loss)\n",
        "* `torch.optim`\n",
        "* `PIL.Image, matplotlib.pyplot, BytesIO, urlopen` (load and display images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sma2QpmZ4f6X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import mse_loss\n",
        "import torchvision.models as models\n",
        "from torchvision.transforms.functional import resize, to_tensor, normalize, to_pil_image\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "from urllib.request import urlopen\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77NiJnTW4f6X"
      },
      "source": [
        "## Loading Images\n",
        "\n",
        "On next section we will load images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texture_imgnames = [\"bois.png\", \"briques.png\", \"mur.png\", \"tissu.png\", \"nuages.png\",\"pebbles.jpg\",\"wall1003.png\"]\n",
        "#import wget\n",
        "for fname in texture_imgnames:\n",
        "  os.system(\"wget -c https://www.idpoisson.fr/galerne/mva/\"+fname)\n",
        "  img = Image.open(fname)\n",
        "  print(img.size)\n",
        "  display(img)"
      ],
      "metadata": {
        "id": "KtVWMSqvca3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6LIdf1qBzZo"
      },
      "source": [
        "## Set a device\n",
        "\n",
        "Next, we need to choose which device to run the network. Running the algorithm on large images takes longer and will go much faster when running on a GPU. We can use `torch.cuda.is_available()` to detect if there is a GPU available. Next, we set the `torch.device`. Also the `.to(device)` method is used to move tensors or modules to a desired device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGx_kjNJByZW"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device is\", device)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01lplU6qAZaF"
      },
      "source": [
        "## Prepare data ##\n",
        "\n",
        "The original PIL RGB images have values between 0 and 255 and size WxHx3, but when transformed into torch tensors, their values are converted to be between 0 and 1 with size 3xWxH. This \"chanel first\" convention is always used to pass an image into a CNN.\n",
        "\n",
        "VGG networks are trained on images with each channel normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] (mean and standard deviation of ImageNet). We will have to normalize the image tensor before sending it into the network.\n",
        "\n",
        "Here some auxiliary functions to load, display and transform to tensors. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wc6VlPt_SdU"
      },
      "outputs": [],
      "source": [
        "# Utilities\n",
        "# Functions to manage images\n",
        "\n",
        "MEAN = (0.485, 0.456, 0.406)\n",
        "STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "def prep_img(imagename: str, size=None, mean=MEAN, std=STD):\n",
        "    \"\"\"Preprocess image.\n",
        "    1) load as PIl\n",
        "    2) resize\n",
        "    3) convert to tensor\n",
        "    4) normalize\n",
        "    \"\"\"\n",
        "    im = Image.open(imagename)\n",
        "    texture = resize(im, size) # resize so that minimal side length is size pixels\n",
        "    texture_tensor = to_tensor(texture).unsqueeze(0) # add batch dimension\n",
        "    # remove alpha channel if any\n",
        "    if texture_tensor.shape[1]==4:\n",
        "      print('removing alpha chanel')\n",
        "      texture_tensor = texture_tensor[:,:3,:,:]\n",
        "    texture_tensor = normalize(texture_tensor, mean=mean, std=std)\n",
        "    return texture_tensor\n",
        "\n",
        "\n",
        "def denormalize(tensor: torch.Tensor, mean=MEAN, std=STD):\n",
        "    \"\"\"Based on torchvision.transforms.functional.normalize.\n",
        "    \"\"\"\n",
        "    tensor = tensor.clone().squeeze() # remove batch dimension\n",
        "    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
        "    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
        "    tensor.mul_(std).add_(mean)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def to_pil(tensor: torch.Tensor):\n",
        "    \"\"\"Converts tensor to PIL Image.\n",
        "    Args: tensor (torch.Temsor): input tensor to be converted to PIL Image of torch.Size([C, H, W]).\n",
        "    Returns: PIL Image: converted img.\n",
        "    \"\"\"\n",
        "    img = tensor.clone().detach().cpu()\n",
        "    img = denormalize(img).clip(0, 1)\n",
        "    img = to_pil_image(img)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgzDtbmEV812"
      },
      "source": [
        "Now, we transform the image to tensor, making the normalization and resize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6UpQBZEEMLb"
      },
      "outputs": [],
      "source": [
        "input_image_name = \"wall1003.png\"\n",
        "img_size = 256\n",
        "\n",
        "# Prepare texture data\n",
        "target = prep_img(input_image_name, img_size).to(device)\n",
        "target_img = to_pil(target)\n",
        "plt.imshow(target_img)\n",
        "display(target_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jW7muFnJ9p9"
      },
      "source": [
        "## Model\n",
        "\n",
        "Now we need to import a pretrained neural network. We will use a 19 layer VGG network of pyTorch.\n",
        "\n",
        "PyTorch implementation of VGG is a module divided into two child Sequential modules: features (containing convolution and pooling layers), and classifier (containing fully connected layers). For the texture synthesis task we only care about the layers of the features module. Also, don't let the parameters change (the network is already trained). \n",
        "\n",
        "On the output of next commands you can see the structure of `features` module. Indexes will help to select the needed layers for the algorithm. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te32Cde1MWaj"
      },
      "outputs": [],
      "source": [
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "cnn.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jCB3teT1GEj"
      },
      "source": [
        "According to the algorithm explained at the beginning of this notebook, we need to access the outputs of some selected intermediate layers.\n",
        "\n",
        "In order to access the outputs of the layers on PyTorch VGG19 network, we need to register a hook on each layer we need. Hooks are functions, able to be attached to every layer and called each time the layer is compited. You can register a hook before or after the forward pass, or after the backward pass. We will define a function `save_output` that will be triggered after the forward pass, for each layer of `features` module. \n",
        "\n",
        "The outputs of the layers will be stored on a dictionary where the key is the index of the layer and the value is the output tensor of the layer.\n",
        "\n",
        "So, we must define which layers will be part of the optimization and define weights for each one (we will use the weights when running the texture synthesis). Using the indexes of layers, we select the layers to use in the algorithm. The output of first conv layer, and the outputs of pools layers are a good selection. That's why we choose indexes 0, 4, 9, 18, 27. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB2sDyreRY7m"
      },
      "outputs": [],
      "source": [
        "# Initialize outputs dic\n",
        "outputs = {}\n",
        "\n",
        "# Hook definition\n",
        "def save_output(name):\n",
        "    \n",
        "    # The hook signature\n",
        "    def hook(module, module_in, module_out):\n",
        "        outputs[name] = module_out\n",
        "    return hook\n",
        "\n",
        "# Define layers\n",
        "layers = [1, 6, 11, 20, 29]\n",
        "# Define weights for layers\n",
        "layers_weights = [1e9/n**2 for n in [64,128,256,512,512]]\n",
        "\n",
        "# Register hook on each layer with index on array \"layers\"\n",
        "for layer in layers:\n",
        "    handle = cnn[layer].register_forward_hook(save_output(layer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a70keSW7YhMD"
      },
      "source": [
        "## Loss Function and optimizer\n",
        "\n",
        "Now, we need to define the Loss function $E$ defined at the beginning of the notebook. To do so we define a function to calculate the Gram Matrix of a feature layer, and then a loss function that computes the Mean-Square-Error (MSE) for 2 Gram matrices. \n",
        "We also compute the Gram matrices of the target once to save computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHKjVMHpYgkd"
      },
      "outputs": [],
      "source": [
        "# Computes Gram matrix for the input batch tensor.\n",
        "#    Args: tnsr (torch.Tensor): input tensor of the Size([B, C, H, W]).\n",
        "#    Returns:  G (torch.Tensor): output tensor of the Size([B, C, C]).\n",
        "def gramm(tnsr: torch.Tensor) -> torch.Tensor:   \n",
        "    b,c,h,w = tnsr.size() \n",
        "    F = tnsr.view(b, c, h*w)\n",
        "    G = torch.bmm(F, F.transpose(1,2)) \n",
        "    G.div_(h*w)\n",
        "    return G\n",
        "\n",
        "# Computes MSE Loss for 2 Gram matrices \n",
        "def gram_loss(input: torch.Tensor, gramm_target: torch.Tensor, weight: float = 1.0):\n",
        "    loss = weight * mse_loss(gramm(input), gramm_target)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer and initialization:\n",
        "\n",
        "Then we compute the random initialization $x_0$. This tensor has to have the same size of the original image.\n",
        "\n",
        "We use L-BFGS algorithm to run our gradient descent. We will create a PyTorch L-BFGS optimizer `optim.LBFGS` and pass our `synth` image to it as the tensor to optimize."
      ],
      "metadata": {
        "id": "fbXfkn9RgLzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selec input image: [\"bois.png\", \"briques.png\", \"mur.png\", \"tissu.png\", \"nuages.png\",\"pebbles.jpg\",\"wall1003.png\"]\n",
        "input_image_name = \"wall1003.png\"\n",
        "img_size = 256\n",
        "\n",
        "# Prepare texture data\n",
        "target = prep_img(input_image_name, img_size).to(device)\n",
        "target_img = to_pil(target)\n",
        "plt.imshow(target_img)\n",
        "\n",
        "# Forward pass using target texture for get activations of selected layers (outputs). Calculate gram Matrix for those activations\n",
        "cnn(target)\n",
        "gramm_targets = [gramm(outputs[key]) for key in layers] \n",
        "\n",
        "# Random init for image synth\n",
        "synth = torch.randn_like(target) * 0.5\n",
        "synth.requires_grad=True\n",
        "\n",
        "# Set optimizer\n",
        "optimizer = optim.LBFGS([synth])"
      ],
      "metadata": {
        "id": "HCpEmSDhgRGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbJGXcY74f6Y"
      },
      "source": [
        "## Running Texture Synthesis\n",
        "\n",
        "Finally, we must run code that performs the texture synthesis. \n",
        "\n",
        "We have to compute the activations of the layers selected for the texture image (dictionary `outputs` after forward pass using target texture). We also will compute for Gram Matrix for those activations (this values doesn't change so is efficient calculate it just once). \n",
        "\n",
        "Then, for each iteration of the network, it is fed an updated input and computes new losses between `target` activations and `synth` activations.\n",
        "\n",
        "The optimizer requires a “closure” function, which reevaluates the module and returns the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isjH-nC94f6Z"
      },
      "outputs": [],
      "source": [
        "n_iters = 2000\n",
        "log_every = n_iters//10\n",
        "iter_ = 0\n",
        "\n",
        "while iter_ <= n_iters:\n",
        "\n",
        "    def closure():\n",
        "        global iter_\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass using synth. Get activations of selected layers for image synth (outputs). Calculate gram Matrix for those activations\n",
        "        cnn(synth)\n",
        "        synth_outputs = [outputs[key] for key in layers] \n",
        "        \n",
        "        # Compute loss for each activation\n",
        "        losses = []\n",
        "        for activations in zip(synth_outputs, gramm_targets, layers_weights):\n",
        "            losses.append(gram_loss(*activations).unsqueeze(0))\n",
        "\n",
        "        total_loss = torch.cat(losses).sum()\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Display results: print Loss value and show image\n",
        "        if iter_ == 0 or iter_ % log_every == 0:\n",
        "            print('Iteration: %d, loss: %1.2e'%(iter_, total_loss.item()))\n",
        "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 20))\n",
        "            axes[0].imshow(target_img)\n",
        "            axes[0].set_title('original image')\n",
        "            axes[1].imshow(to_pil(synth))\n",
        "            axes[1].set_title('synthesis (it. %d)'%( iter_ ))\n",
        "            fig.tight_layout()\n",
        "            plt.pause(0.05)\n",
        "\n",
        "        iter_ += 1\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    optimizer.step(closure)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2:\n",
        "\n",
        "Pick one of the following problems:\n",
        "\n",
        "* **A: Color correction** \n",
        "  \n",
        "  Observe that for some image the color is inconsistant (eg with `wall1003.png`). A solution to correct the output color distribution is to incorporate the mean color and the color covariance as a target statistics in $E$. \n",
        "\n",
        "  **Hint**: Considering mean color vector $m$ and covariance matices $C_h = C_h(0)$:\n",
        "\n",
        "  $$\n",
        "  m = \\begin{pmatrix}\n",
        "  m_r \\\\\n",
        "  m_g \\\\\n",
        "  m_b\n",
        "  \\end{pmatrix}\n",
        "  = \\frac{1}{MN}\\sum_{t\\in\\Omega} h(t) \\in \\mathbb{R}^{3}\n",
        "  $$\n",
        "  $$\n",
        "  C_h = \\frac{1}{MN}\\sum_{t\\in\\Omega} \n",
        "  \\begin{pmatrix}\n",
        "  h_r(t) - m_r \\\\\n",
        "  h_g(t) - m_g \\\\\n",
        "  h_b(t) - m_b\n",
        "  \\end{pmatrix}\n",
        "  \\begin{pmatrix}\n",
        "  h_r(t) - m_r \\\\\n",
        "  h_g(t) - m_g \\\\\n",
        "  h_b(t) - m_b\n",
        "  \\end{pmatrix}^T\n",
        "  \\in\\mathbb{R}^{3\\times 3}.\n",
        "  $$\n",
        "\n",
        "  Then change $E$ to:\n",
        "  $ E + \\lambda_{mean} \\| m(x) - m(u)\\|^2 + \\lambda_{cov} \\| C(x) - C(u)\\|^2. $\n",
        "\n",
        "  Try with $\\lambda_{mean}$ and $\\lambda_{cov}$ between 1e6 and 1e3.\n",
        "\n",
        "\n",
        "\n",
        "* **B: Spectral correction** \n",
        "\n",
        "  Add a term to the energy that would enforce a consistency with the original Fourier spectrum of each color channel, that is change $E$ to: \n",
        "$$\n",
        "E + \\lambda_{Fourier} \\| |\\hat{x}| - |\\hat{u}|\\|^2.\n",
        "$$\n",
        "Try with $\\lambda_{Fourier}$ between 5 and 0.5 to see the differences. Try with different kind of textures.\n",
        "What is the interest of this approach?\n",
        "What are the textures for which it improves or degrades the quality of the result? \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **C: Order one statistics** \n",
        "\n",
        "  Replace $E$ so that the spatial average of ALL the VGG-19 layers is preserved, that is change $E$ to:\n",
        "$$\n",
        "  E_{mean} (x) = \\sum_{\\text{for all layers } L} w_L \\left\\| \\operatorname{mean}(V^L(x)) - \\operatorname{mean}(V^L(u)) \\right\\|^2_F\n",
        "$$\n",
        "  Here, consider weighting the layers with the same approach made in the notebook. The mean is computed along the spatial dimension, so for each layer the mean vector has size \"number of channels within the layer\". \n",
        " Compare with the original model. What is the interest of this approach?\n",
        "\n",
        "  \n",
        "  \n"
      ],
      "metadata": {
        "id": "gAHa88q3sTyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CNTOPsx6usBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "2_mvagm_CNN_texture_synthesis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}